{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0m4x9M5JRus2"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import logging\n",
        "import zipfile\n",
        "import subprocess\n",
        "import shutil\n",
        "import glob\n",
        "from datetime import datetime, timedelta\n",
        "from pathlib import Path\n",
        "from typing import List, Optional\n",
        "\n",
        "class RINEXConverter:\n",
        "    def __init__(self):\n",
        "        self.setup_directories()\n",
        "        self.setup_logging()\n",
        "        self.hour_letters = {i: chr(ord('a') + i) for i in range(24)}\n",
        "        self.navigation_types = ['n', 'g', 'l', 'q', 'c', 'j', 'i', 'h']\n",
        "\n",
        "    def setup_directories(self):\n",
        "        self.base_dir = Path(\"C:/RNXConverter\")\n",
        "        self.input_dir = self.base_dir / \"Input\"\n",
        "        self.input_raw_dir = self.base_dir / \"Input\" / \"Raw\"\n",
        "        self.temp_dir = self.base_dir / \"temp\"\n",
        "        self.final_dir = self.base_dir / \"Output\" / \"Final\"\n",
        "\n",
        "        # Create directories\n",
        "        self.input_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.input_raw_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.temp_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.final_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def setup_logging(self):\n",
        "        log_file = self.base_dir / \"rinex_converter.log\"\n",
        "        self.logger = logging.getLogger('RINEXConverter')\n",
        "        self.logger.setLevel(logging.DEBUG)\n",
        "\n",
        "        # Clear existing handlers\n",
        "        self.logger.handlers.clear()\n",
        "\n",
        "        file_handler = logging.FileHandler(log_file, mode='a')\n",
        "        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
        "        self.logger.addHandler(file_handler)\n",
        "\n",
        "        console_handler = logging.StreamHandler(sys.stdout)\n",
        "        console_handler.setFormatter(logging.Formatter('%(levelname)s: %(message)s'))\n",
        "        self.logger.addHandler(console_handler)\n",
        "\n",
        "        self.logger.info(\"================ RINEX CONVERTER START ================\")\n",
        "\n",
        "    def get_user_input(self) -> dict:\n",
        "        config = {}\n",
        "\n",
        "        # Get station inputs\n",
        "        leica_sites = input(\"Leica site/s (comma-separated, lowercase): \").strip()\n",
        "        config['leica_sites'] = [site.strip() for site in leica_sites.split(',') if site.strip()]\n",
        "\n",
        "        trimble_sites = input(\"Trimble site/s (comma-separated, uppercase): \").strip()\n",
        "        config['trimble_sites'] = [site.strip() for site in trimble_sites.split(',') if site.strip()]\n",
        "\n",
        "        # Get date range\n",
        "        start_date = input(\"Start date (dd mm yyyy): \").strip()\n",
        "        end_date = input(\"End date (dd mm yyyy): \").strip()\n",
        "        config['start_date'] = datetime.strptime(start_date, \"%d %m %Y\")\n",
        "        config['end_date'] = datetime.strptime(end_date, \"%d %m %Y\")\n",
        "\n",
        "        # Get time range\n",
        "        start_time = input(\"UTC start time (hhmm): \").strip()\n",
        "        end_time = input(\"UTC end time (hhmm): \").strip()\n",
        "        config['start_hour'] = int(start_time[:2])\n",
        "        config['start_minute'] = int(start_time[2:])\n",
        "        config['end_hour'] = int(end_time[:2])\n",
        "        config['end_minute'] = int(end_time[2:])\n",
        "\n",
        "        # Get processing options\n",
        "        config['logging_interval'] = int(input(\"Logging interval in seconds: \").strip())\n",
        "        config['rinex_version'] = input(\"RINEX version (2/3): \").strip()\n",
        "        config['include_nav'] = input(\"Include navigation files? (y/n): \").strip().lower() == 'y'\n",
        "        config['hatanaka_compression'] = input(\"Apply Hatanaka compression? (y/n): \").strip().lower() == 'y'\n",
        "\n",
        "        return config\n",
        "\n",
        "    def scan_for_zip_files(self):\n",
        "        \"\"\"Scan and list all zip files in input directories for debugging\"\"\"\n",
        "        self.logger.info(\"=== SCANNING FOR ZIP FILES ===\")\n",
        "\n",
        "        # Check main input directory\n",
        "        input_files = list(self.input_dir.glob(\"*.zip\"))\n",
        "        self.logger.info(f\"Files in {self.input_dir}:\")\n",
        "        for file in input_files:\n",
        "            self.logger.info(f\"  - {file.name}\")\n",
        "\n",
        "        # Check input/raw directory\n",
        "        raw_files = list(self.input_raw_dir.glob(\"*.zip\"))\n",
        "        self.logger.info(f\"Files in {self.input_raw_dir}:\")\n",
        "        for file in raw_files:\n",
        "            self.logger.info(f\"  - {file.name}\")\n",
        "\n",
        "        # Check if files are in Raw directory and move them to Input\n",
        "        if raw_files and not input_files:\n",
        "            self.logger.info(\"Found zip files in Raw directory, moving to Input directory...\")\n",
        "            for file in raw_files:\n",
        "                new_location = self.input_dir / file.name\n",
        "                shutil.move(str(file), str(new_location))\n",
        "                self.logger.info(f\"Moved {file.name} to Input directory\")\n",
        "\n",
        "        self.logger.info(\"=== END SCAN ===\")\n",
        "\n",
        "    def find_and_extract_zip_files(self, config: dict) -> List[dict]:\n",
        "        \"\"\"Find and extract zip files matching the criteria\"\"\"\n",
        "        extracted_files = []\n",
        "\n",
        "        # Debug: List all files in input directory\n",
        "        self.logger.info(f\"Scanning input directory: {self.input_dir}\")\n",
        "        all_files = list(self.input_dir.glob(\"*\"))\n",
        "        self.logger.info(f\"Found {len(all_files)} files in input directory:\")\n",
        "        for file in all_files:\n",
        "            self.logger.info(f\"  - {file.name}\")\n",
        "\n",
        "        current_date = config['start_date']\n",
        "        end_date = config['end_date']\n",
        "\n",
        "        while current_date <= end_date:\n",
        "            doy = current_date.timetuple().tm_yday\n",
        "            year = current_date.year\n",
        "\n",
        "            # Determine hour range for this day\n",
        "            if current_date == config['start_date']:\n",
        "                start_hour = config['start_hour']\n",
        "            else:\n",
        "                start_hour = 0\n",
        "\n",
        "            if current_date == config['end_date']:\n",
        "                end_hour = config['end_hour']\n",
        "                if config['end_minute'] == 0:\n",
        "                    end_hour -= 1\n",
        "            else:\n",
        "                end_hour = 23\n",
        "\n",
        "            # Process each hour\n",
        "            for hour in range(start_hour, end_hour + 1):\n",
        "                hour_letter = self.hour_letters[hour]\n",
        "                doy_str = f\"{doy:03d}\"\n",
        "\n",
        "                # Check for Leica files\n",
        "                for site in config['leica_sites']:\n",
        "                    if site:  # Only process if site is not empty\n",
        "                        zip_pattern = f\"{site}{doy_str}{hour_letter}.zip\"\n",
        "                        self.logger.info(f\"Looking for Leica pattern: {zip_pattern}\")\n",
        "                        zip_files = list(self.input_dir.glob(zip_pattern))\n",
        "\n",
        "                        if not zip_files:\n",
        "                            # Also try with different extensions or patterns\n",
        "                            alt_patterns = [\n",
        "                                f\"{site}{doy_str}{hour_letter}.*.zip\",\n",
        "                                f\"{site.upper()}{doy_str}{hour_letter}.zip\",\n",
        "                                f\"{site.lower()}{doy_str}{hour_letter}.zip\"\n",
        "                            ]\n",
        "                            for alt_pattern in alt_patterns:\n",
        "                                alt_files = list(self.input_dir.glob(alt_pattern))\n",
        "                                if alt_files:\n",
        "                                    self.logger.info(f\"Found files with alternative pattern: {alt_pattern}\")\n",
        "                                    zip_files.extend(alt_files)\n",
        "                                    break\n",
        "\n",
        "                        for zip_file in zip_files:\n",
        "                            self.logger.info(f\"Extracting Leica file: {zip_file.name}\")\n",
        "                            with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "                                z.extractall(self.input_raw_dir)\n",
        "\n",
        "                            extracted_files.append({\n",
        "                                'site': site,\n",
        "                                'doy': doy,\n",
        "                                'hour': hour,\n",
        "                                'year': year,\n",
        "                                'type': 'leica',\n",
        "                                'zip_file': zip_file\n",
        "                            })\n",
        "\n",
        "                # Check for Trimble files\n",
        "                for site in config['trimble_sites']:\n",
        "                    if site:  # Only process if site is not empty\n",
        "                        zip_pattern = f\"{site}{doy_str}{hour_letter}.zip\"\n",
        "                        self.logger.info(f\"Looking for Trimble pattern: {zip_pattern}\")\n",
        "                        zip_files = list(self.input_dir.glob(zip_pattern))\n",
        "\n",
        "                        if not zip_files:\n",
        "                            # Also try with different extensions or patterns\n",
        "                            alt_patterns = [\n",
        "                                f\"{site}{doy_str}{hour_letter}.*.zip\",\n",
        "                                f\"{site.upper()}{doy_str}{hour_letter}.zip\",\n",
        "                                f\"{site.lower()}{doy_str}{hour_letter}.zip\"\n",
        "                            ]\n",
        "                            for alt_pattern in alt_patterns:\n",
        "                                alt_files = list(self.input_dir.glob(alt_pattern))\n",
        "                                if alt_files:\n",
        "                                    self.logger.info(f\"Found files with alternative pattern: {alt_pattern}\")\n",
        "                                    zip_files.extend(alt_files)\n",
        "                                    break\n",
        "\n",
        "                        for zip_file in zip_files:\n",
        "                            self.logger.info(f\"Extracting Trimble file: {zip_file.name}\")\n",
        "                            with zipfile.ZipFile(zip_file, 'r') as z:\n",
        "                                z.extractall(self.input_raw_dir)\n",
        "\n",
        "                            extracted_files.append({\n",
        "                                'site': site,\n",
        "                                'doy': doy,\n",
        "                                'hour': hour,\n",
        "                                'year': year,\n",
        "                                'type': 'trimble',\n",
        "                                'zip_file': zip_file\n",
        "                            })\n",
        "\n",
        "            current_date += timedelta(days=1)\n",
        "\n",
        "        self.logger.info(f\"Total extracted files: {len(extracted_files)}\")\n",
        "        return extracted_files\n",
        "\n",
        "    def merge_t02_files(self, site: str, doy: int, year: int):\n",
        "        \"\"\"Merge T02 files of the same station and DOY\"\"\"\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "        pattern = f\"{site}{doy_str}*.T02\"\n",
        "        t02_files = list(self.input_raw_dir.glob(pattern))\n",
        "\n",
        "        if len(t02_files) > 1:\n",
        "            self.logger.info(f\"Merging {len(t02_files)} T02 files for {site} DOY {doy}\")\n",
        "            merged_file = self.input_raw_dir / f\"{site}{doy_str}.T02\"\n",
        "\n",
        "            with open(merged_file, 'wb') as outfile:\n",
        "                for t02_file in sorted(t02_files):\n",
        "                    with open(t02_file, 'rb') as infile:\n",
        "                        outfile.write(infile.read())\n",
        "                    t02_file.unlink()  # Delete original file\n",
        "\n",
        "            self.logger.info(f\"Merged T02 file created: {merged_file.name}\")\n",
        "            return merged_file\n",
        "        elif len(t02_files) == 1:\n",
        "            return t02_files[0]\n",
        "        else:\n",
        "            self.logger.warning(f\"No T02 files found for {site} DOY {doy}\")\n",
        "            return None\n",
        "\n",
        "    def process_leica_files(self, site: str, doy: int, year: int):\n",
        "        \"\"\"Process Leica MDB files - concatenate all hour files for a full day\"\"\"\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "\n",
        "        # Look for all MDB files for this site and DOY (all hours)\n",
        "        mdb_pattern = f\"{site}{doy_str}*.m*\"\n",
        "        mdb_files = list(self.input_raw_dir.glob(mdb_pattern))\n",
        "\n",
        "        if not mdb_files:\n",
        "            self.logger.warning(f\"No MDB files found for pattern: {mdb_pattern}\")\n",
        "            return\n",
        "\n",
        "        self.logger.info(f\"Processing {len(mdb_files)} Leica MDB files for {site} DOY {doy}\")\n",
        "        for mdb_file in mdb_files:\n",
        "            self.logger.info(f\"  - {mdb_file.name}\")\n",
        "\n",
        "        # Create a space-separated string of all MDB filenames for the command\n",
        "        mdb_filenames = \" \".join([mdb_file.name for mdb_file in mdb_files])\n",
        "\n",
        "        # Run mdb2rinex with all files to create a concatenated full-day file\n",
        "        cmd = f\"mdb2rinex -f {mdb_filenames} -o temp\"\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(cmd, shell=True, cwd=self.input_raw_dir,\n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            self.logger.info(f\"mdb2rinex completed for {site} DOY {doy} (full day)\")\n",
        "            if result.stdout:\n",
        "                self.logger.info(f\"STDOUT: {result.stdout}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            self.logger.error(f\"Error running mdb2rinex: {e}\")\n",
        "            self.logger.error(f\"STDERR: {e.stderr}\")\n",
        "            # Fallback: try processing files individually if batch processing fails\n",
        "            self.logger.info(\"Attempting to process files individually as fallback...\")\n",
        "            for mdb_file in mdb_files:\n",
        "                try:\n",
        "                    cmd_individual = [\"mdb2rinex\", \"-f\", mdb_file.name, \"-o\", \"temp\"]\n",
        "                    result = subprocess.run(cmd_individual, cwd=self.input_raw_dir,\n",
        "                                          capture_output=True, text=True, check=True)\n",
        "                    self.logger.info(f\"Individual processing completed for {mdb_file.name}\")\n",
        "                except subprocess.CalledProcessError as e_individual:\n",
        "                    self.logger.error(f\"Error processing {mdb_file.name}: {e_individual}\")\n",
        "\n",
        "    def process_trimble_files(self, site: str, doy: int, year: int):\n",
        "        \"\"\"Process Trimble T02 files\"\"\"\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "\n",
        "        # First merge T02 files if multiple exist\n",
        "        t02_file = self.merge_t02_files(site, doy, year)\n",
        "\n",
        "        if not t02_file:\n",
        "            return\n",
        "\n",
        "        self.logger.info(f\"Processing Trimble file: {t02_file.name}\")\n",
        "\n",
        "        cmd = [\"convertToRinex\", t02_file.name, \"-v\", \"3.02\", \"-p\", \"temp\"]\n",
        "\n",
        "        try:\n",
        "            result = subprocess.run(cmd, cwd=self.input_raw_dir,\n",
        "                                  capture_output=True, text=True, check=True)\n",
        "            self.logger.info(f\"convertToRinex completed for {t02_file.name}\")\n",
        "            if result.stdout:\n",
        "                self.logger.info(f\"STDOUT: {result.stdout}\")\n",
        "        except subprocess.CalledProcessError as e:\n",
        "            self.logger.error(f\"Error running convertToRinex: {e}\")\n",
        "            self.logger.error(f\"STDERR: {e.stderr}\")\n",
        "\n",
        "    def fix_rinex_headers(self, site: str, doy: int, year: int):\n",
        "        \"\"\"Fix RINEX headers using gfzrnx\"\"\"\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "        year_str = f\"{year % 100:02d}\"\n",
        "\n",
        "        temp_dir = self.input_raw_dir / \"temp\"\n",
        "        if not temp_dir.exists():\n",
        "            self.logger.warning(f\"Temp directory not found: {temp_dir}\")\n",
        "            return\n",
        "\n",
        "        # Find RINEX files in temp directory\n",
        "        rinex_pattern = f\"{site}{doy_str}*.{year_str}o\"\n",
        "        rinex_files = list(temp_dir.glob(rinex_pattern))\n",
        "\n",
        "        if not rinex_files:\n",
        "            self.logger.warning(f\"No RINEX files found for pattern: {rinex_pattern}\")\n",
        "            return\n",
        "\n",
        "        header_file = self.base_dir / \"pagenetHDR.txt\"\n",
        "\n",
        "        for rinex_file in rinex_files:\n",
        "            output_file = temp_dir / f\"fixed_{rinex_file.name}\"\n",
        "\n",
        "            if header_file.exists():\n",
        "                cmd = [\"gfzrnx\", \"-finp\", rinex_file.name, \"-fout\", output_file.name,\n",
        "                       \"-crux\", str(header_file), \"-hded\", \"-q\"]\n",
        "\n",
        "                try:\n",
        "                    result = subprocess.run(cmd, cwd=temp_dir,\n",
        "                                          capture_output=True, text=True, check=True)\n",
        "                    self.logger.info(f\"Header fixed for {rinex_file.name}\")\n",
        "\n",
        "                    # Replace original with fixed version\n",
        "                    rinex_file.unlink()\n",
        "                    output_file.rename(rinex_file)\n",
        "\n",
        "                except subprocess.CalledProcessError as e:\n",
        "                    self.logger.error(f\"Error fixing headers: {e}\")\n",
        "                    self.logger.error(f\"STDERR: {e.stderr}\")\n",
        "            else:\n",
        "                self.logger.warning(\"Header file not found, skipping header fix\")\n",
        "\n",
        "    def convert_rinex_version(self, site: str, doy: int, year: int, target_version: str):\n",
        "        \"\"\"Convert RINEX to specified version\"\"\"\n",
        "        if target_version == \"3\":\n",
        "            return  # Already in version 3\n",
        "\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "        year_str = f\"{year % 100:02d}\"\n",
        "\n",
        "        temp_dir = self.input_raw_dir / \"temp\"\n",
        "        rinex_pattern = f\"{site}{doy_str}*.{year_str}o\"\n",
        "        rinex_files = list(temp_dir.glob(rinex_pattern))\n",
        "\n",
        "        for rinex_file in rinex_files:\n",
        "            output_file = temp_dir / f\"{rinex_file.stem}_rx2{rinex_file.suffix}\"\n",
        "\n",
        "            cmd = [\"gfzrnx\", \"-finp\", rinex_file.name, \"-fout\", output_file.name, \"-vo\", \"2\"]\n",
        "\n",
        "            try:\n",
        "                result = subprocess.run(cmd, cwd=temp_dir,\n",
        "                                      capture_output=True, text=True, check=True)\n",
        "                self.logger.info(f\"Converted to RINEX v2: {rinex_file.name}\")\n",
        "\n",
        "                # Replace original with converted version\n",
        "                rinex_file.unlink()\n",
        "                output_file.rename(rinex_file)\n",
        "\n",
        "            except subprocess.CalledProcessError as e:\n",
        "                self.logger.error(f\"Error converting RINEX version: {e}\")\n",
        "                self.logger.error(f\"STDERR: {e.stderr}\")\n",
        "\n",
        "    def create_zip_files(self, site: str, doy: int, year: int, include_nav: bool):\n",
        "        \"\"\"Create zip files with RINEX data using the new naming convention\"\"\"\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "        year_str = f\"{year % 100:02d}\"\n",
        "\n",
        "        temp_dir = self.input_raw_dir / \"temp\"\n",
        "\n",
        "        # Find observation files\n",
        "        obs_pattern = f\"{site}{doy_str}*.{year_str}o\"\n",
        "        obs_files = list(temp_dir.glob(obs_pattern))\n",
        "\n",
        "        for obs_file in obs_files:\n",
        "            # Create new zip name in format: sitenamedoy.yro.zip (e.g., PAPI0950.23o.zip)\n",
        "            # Extract site name (first 4 characters) and use uppercase\n",
        "            site_name = site[:4].upper()\n",
        "            zip_name = f\"{site_name}{doy_str}0.{year_str}o.zip\"\n",
        "            zip_path = self.final_dir / zip_name\n",
        "\n",
        "            files_to_zip = [obs_file]\n",
        "\n",
        "            if include_nav:\n",
        "                # Find corresponding navigation files\n",
        "                base_name = obs_file.stem\n",
        "                for nav_type in self.navigation_types:\n",
        "                    nav_pattern = f\"{base_name}.{year_str}{nav_type}\"\n",
        "                    nav_files = list(temp_dir.glob(nav_pattern))\n",
        "                    files_to_zip.extend(nav_files)\n",
        "\n",
        "            # Create zip file using 7zip for better compression\n",
        "            # First try 7zip, fallback to Python zipfile if 7zip is not available\n",
        "            try:\n",
        "                # Create file list for 7zip\n",
        "                file_list = []\n",
        "                for file_path in files_to_zip:\n",
        "                    if file_path.exists():\n",
        "                        file_list.append(file_path.name)\n",
        "\n",
        "                if file_list:\n",
        "                    # Use 7zip command\n",
        "                    cmd = [\"7z\", \"a\", \"-tzip\", str(zip_path)] + file_list\n",
        "                    result = subprocess.run(cmd, cwd=temp_dir,\n",
        "                                          capture_output=True, text=True, check=True)\n",
        "                    self.logger.info(f\"Created zip file with 7zip: {zip_path.name}\")\n",
        "                    for file_name in file_list:\n",
        "                        self.logger.info(f\"Added to zip: {file_name}\")\n",
        "\n",
        "            except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
        "                # Fallback to Python zipfile if 7zip fails or is not available\n",
        "                self.logger.warning(f\"7zip not available or failed: {e}. Using Python zipfile as fallback.\")\n",
        "\n",
        "                with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                    for file_path in files_to_zip:\n",
        "                        if file_path.exists():\n",
        "                            zipf.write(file_path, file_path.name)\n",
        "                            self.logger.info(f\"Added to zip: {file_path.name}\")\n",
        "\n",
        "                self.logger.info(f\"Created zip file with Python zipfile: {zip_path.name}\")\n",
        "\n",
        "    def apply_hatanaka_compression(self, site: str, doy: int, year: int):\n",
        "        \"\"\"Apply Hatanaka compression to RINEX files\"\"\"\n",
        "        doy_str = f\"{doy:03d}\"\n",
        "        year_str = f\"{year % 100:02d}\"\n",
        "\n",
        "        # Find zip files in final directory using the new naming convention\n",
        "        site_name = site[:4].upper()\n",
        "        zip_pattern = f\"{site_name}{doy_str}0.{year_str}o.zip\"\n",
        "        zip_files = list(self.final_dir.glob(zip_pattern))\n",
        "\n",
        "        for zip_file in zip_files:\n",
        "            # Extract zip file temporarily\n",
        "            temp_extract_dir = self.final_dir / \"temp_extract\"\n",
        "            temp_extract_dir.mkdir(exist_ok=True)\n",
        "\n",
        "            with zipfile.ZipFile(zip_file, 'r') as zipf:\n",
        "                zipf.extractall(temp_extract_dir)\n",
        "\n",
        "            # Find observation files and compress them\n",
        "            obs_files = list(temp_extract_dir.glob(f\"*.{year_str}o\"))\n",
        "\n",
        "            for obs_file in obs_files:\n",
        "                compressed_file = temp_extract_dir / f\"{obs_file.stem}.{year_str}d\"\n",
        "\n",
        "                # Apply Hatanaka compression\n",
        "                cmd = f\"gfzrnx -finp {obs_file.name} | rnx2crx > {compressed_file.name}\"\n",
        "\n",
        "                try:\n",
        "                    result = subprocess.run(cmd, shell=True, cwd=temp_extract_dir,\n",
        "                                          capture_output=True, text=True, check=True)\n",
        "                    self.logger.info(f\"Applied Hatanaka compression: {obs_file.name}\")\n",
        "\n",
        "                    # Remove uncompressed file\n",
        "                    obs_file.unlink()\n",
        "\n",
        "                except subprocess.CalledProcessError as e:\n",
        "                    self.logger.error(f\"Error applying Hatanaka compression: {e}\")\n",
        "                    self.logger.error(f\"STDERR: {e.stderr}\")\n",
        "\n",
        "            # Create new zip file with compressed data using the same naming convention\n",
        "            compressed_zip_name = zip_file.name.replace('.zip', '_compressed.zip')\n",
        "            compressed_zip_path = self.final_dir / compressed_zip_name\n",
        "\n",
        "            # Try to create compressed zip with 7zip first, fallback to Python zipfile\n",
        "            try:\n",
        "                # Create file list for 7zip\n",
        "                file_list = []\n",
        "                for file_path in temp_extract_dir.glob(\"*\"):\n",
        "                    if file_path.is_file():\n",
        "                        file_list.append(file_path.name)\n",
        "\n",
        "                if file_list:\n",
        "                    # Use 7zip command\n",
        "                    cmd = [\"7z\", \"a\", \"-tzip\", str(compressed_zip_path)] + file_list\n",
        "                    result = subprocess.run(cmd, cwd=temp_extract_dir,\n",
        "                                          capture_output=True, text=True, check=True)\n",
        "                    self.logger.info(f\"Created compressed zip with 7zip: {compressed_zip_path.name}\")\n",
        "\n",
        "            except (subprocess.CalledProcessError, FileNotFoundError) as e:\n",
        "                # Fallback to Python zipfile if 7zip fails or is not available\n",
        "                self.logger.warning(f\"7zip not available or failed: {e}. Using Python zipfile as fallback.\")\n",
        "\n",
        "                with zipfile.ZipFile(compressed_zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "                    for file_path in temp_extract_dir.glob(\"*\"):\n",
        "                        if file_path.is_file():\n",
        "                            zipf.write(file_path, file_path.name)\n",
        "\n",
        "                self.logger.info(f\"Created compressed zip with Python zipfile: {compressed_zip_path.name}\")\n",
        "\n",
        "            # Remove original zip and temp directory\n",
        "            zip_file.unlink()\n",
        "            shutil.rmtree(temp_extract_dir)\n",
        "\n",
        "    def cleanup_temporary_files(self):\n",
        "        \"\"\"Clean up all temporary files\"\"\"\n",
        "        self.logger.info(\"Cleaning up temporary files...\")\n",
        "\n",
        "        # Clean Input/Raw directory\n",
        "        for file_path in self.input_raw_dir.glob(\"*\"):\n",
        "            if file_path.is_file():\n",
        "                file_path.unlink()\n",
        "            elif file_path.is_dir():\n",
        "                shutil.rmtree(file_path)\n",
        "\n",
        "        # Clean temp directory\n",
        "        if self.temp_dir.exists():\n",
        "            shutil.rmtree(self.temp_dir)\n",
        "\n",
        "        self.logger.info(\"Cleanup completed\")\n",
        "\n",
        "    def run(self):\n",
        "        config = self.get_user_input()\n",
        "\n",
        "        # Debug: Scan for zip files first\n",
        "        self.scan_for_zip_files()\n",
        "\n",
        "        # Step 1: Find and extract zip files\n",
        "        self.logger.info(\"Step 1: Finding and extracting zip files...\")\n",
        "        extracted_files = self.find_and_extract_zip_files(config)\n",
        "\n",
        "        if not extracted_files:\n",
        "            self.logger.warning(\"No files found for processing\")\n",
        "            self.logger.info(\"Please check:\")\n",
        "            self.logger.info(\"1. Zip files are in C:\\\\RNXConverter\\\\Input\\\\\")\n",
        "            self.logger.info(\"2. File names follow the pattern: sitenameDOYhour.zip\")\n",
        "            self.logger.info(\"3. DOY is 3 digits (e.g., 095 for day 95)\")\n",
        "            self.logger.info(\"4. Hour is a letter (a-x for hours 0-23)\")\n",
        "            self.logger.info(\"5. Date range and stations match your input\")\n",
        "            return\n",
        "\n",
        "        # Group files by site and DOY for processing\n",
        "        site_groups = {}\n",
        "        for file_info in extracted_files:\n",
        "            key = (file_info['site'], file_info['doy'], file_info['year'], file_info['type'])\n",
        "            if key not in site_groups:\n",
        "                site_groups[key] = []\n",
        "            site_groups[key].append(file_info)\n",
        "\n",
        "        # Process each site group\n",
        "        for (site, doy, year, site_type), files in site_groups.items():\n",
        "            self.logger.info(f\"Processing {site} DOY {doy} ({site_type})\")\n",
        "\n",
        "            # Step 2-4: Process raw files\n",
        "            if site_type == 'leica':\n",
        "                self.process_leica_files(site, doy, year)\n",
        "            elif site_type == 'trimble':\n",
        "                self.process_trimble_files(site, doy, year)\n",
        "\n",
        "            # Step 5: Fix headers\n",
        "            self.fix_rinex_headers(site, doy, year)\n",
        "\n",
        "            # Step 6: Convert RINEX version if needed\n",
        "            if config['rinex_version'] == '2':\n",
        "                self.convert_rinex_version(site, doy, year, '2')\n",
        "\n",
        "            # Step 6a: Create zip files\n",
        "            self.create_zip_files(site, doy, year, config['include_nav'])\n",
        "\n",
        "            # Step 7: Apply Hatanaka compression if requested\n",
        "            if config['hatanaka_compression']:\n",
        "                self.apply_hatanaka_compression(site, doy, year)\n",
        "\n",
        "        # Step 8-9: Cleanup\n",
        "        self.cleanup_temporary_files()\n",
        "\n",
        "        self.logger.info(\"Processing completed successfully!\")\n",
        "\n",
        "def main():\n",
        "    converter = RINEXConverter()\n",
        "    converter.run()\n",
        "    input(\"\\nPress Enter to exit...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}